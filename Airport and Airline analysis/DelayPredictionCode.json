{"paragraphs":[{"text":"//Importing all the required libraries\n\nimport org.apache.spark.ml.feature.VectorIndexer\n\nimport org.apache.spark.mllib.tree.configuration.BoostingStrategy\n\nimport org.apache.spark.mllib.tree.{GradientBoostedTrees, RandomForest}\n\nimport org.apache.spark.ml.feature.StringIndexer\n\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\n\nimport org.apache.spark.sql.{SaveMode, DataFrame, SQLContext, Row}","user":"anonymous","dateUpdated":"2017-05-10T00:29:25-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.ml.feature.VectorIndexer\n\nimport org.apache.spark.mllib.tree.configuration.BoostingStrategy\n\nimport org.apache.spark.mllib.tree.{GradientBoostedTrees, RandomForest}\n\nimport org.apache.spark.ml.feature.StringIndexer\n\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\n\nimport org.apache.spark.sql.{SaveMode, DataFrame, SQLContext, Row}\n"}]},"apps":[],"jobName":"paragraph_1493576688684_1737475769","id":"20170430-142448_334886702","dateCreated":"2017-04-30T14:24:48-0400","dateStarted":"2017-05-10T00:29:26-0400","dateFinished":"2017-05-10T00:29:58-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:666"},{"text":"//Creating SparkSession to Load csv\n\nval spark = org.apache.spark.sql.SparkSession.builder\n        .master(\"local\")\n        .appName(\"Spark CSV Reader\")\n        .getOrCreate;","user":"anonymous","dateUpdated":"2017-05-10T00:29:26-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nspark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@798f1f2f\n"}]},"apps":[],"jobName":"paragraph_1493577794545_1379591941","id":"20170430-144314_1239889849","dateCreated":"2017-04-30T14:43:14-0400","dateStarted":"2017-05-10T00:29:29-0400","dateFinished":"2017-05-10T00:29:59-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:667"},{"text":"// Function to load the Dataset\n\ndef loadData(filePath:String, bool:String): DataFrame={\n        spark.read\n        .format(\"com.databricks.spark.csv\")\n        .option(\"header\", bool) //reading the headers\n        .option(\"mode\", \"DROPMALFORMED\")\n        .load(filePath); //.csv(\"csv/file/path\") //spark 2.0 ap\n}","user":"anonymous","dateUpdated":"2017-05-10T00:29:26-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nloadData: (filePath: String, bool: String)org.apache.spark.sql.DataFrame\n"}]},"apps":[],"jobName":"paragraph_1493577620847_-1296011563","id":"20170430-144020_1410940717","dateCreated":"2017-04-30T14:40:20-0400","dateStarted":"2017-05-10T00:29:59-0400","dateFinished":"2017-05-10T00:30:00-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:668"},{"text":"// Function to combine Flight Data\ndef combineFlight(Dataset1 : DataFrame, Dataset2 : DataFrame) : DataFrame={\n    \n    Dataset1.registerTempTable(\"flightDataset1\");\n    Dataset2.registerTempTable(\"flightDataset2\");\n    \n    spark.sql(\"Select * from flightDataset1 UNION Select * from flightDataset2\");\n    \n}","user":"anonymous","dateUpdated":"2017-05-10T00:29:26-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nwarning: there were two deprecation warnings; re-run with -deprecation for details\n\ncombineFlight: (Dataset1: org.apache.spark.sql.DataFrame, Dataset2: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"}]},"apps":[],"jobName":"paragraph_1493582772742_1128826482","id":"20170430-160612_56103398","dateCreated":"2017-04-30T16:06:12-0400","dateStarted":"2017-05-10T00:29:59-0400","dateFinished":"2017-05-10T00:30:00-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:669"},{"text":"// Function to combine Weather Data\ndef combineWeather(Dataset1: DataFrame, Dataset2: DataFrame, Dataset3: DataFrame): DataFrame = {\n    \n    Dataset1.registerTempTable(\"weatherDataset1\");\n    Dataset2.registerTempTable(\"weatherDataset2\");\n    Dataset3.registerTempTable(\"weatherDataset3\");\n    \n    val weather_data_temp = spark.sql(\"select * from weatherDataset1 union select * from weatherDataset2 union select * from weatherDataset3\");\n\n    weather_data_temp.registerTempTable(\"combinedWeatherData\");\n\n    spark.sql(\"select * from combinedWeatherData where _c1=2006 OR _c1=2007\");\n    \n}","user":"anonymous","dateUpdated":"2017-05-10T00:29:26-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nwarning: there were four deprecation warnings; re-run with -deprecation for details\n\ncombineWeather: (Dataset1: org.apache.spark.sql.DataFrame, Dataset2: org.apache.spark.sql.DataFrame, Dataset3: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"}]},"apps":[],"jobName":"paragraph_1493583007290_-2058496347","id":"20170430-161007_976444735","dateCreated":"2017-04-30T16:10:07-0400","dateStarted":"2017-05-10T00:30:00-0400","dateFinished":"2017-05-10T00:30:01-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:670"},{"text":"//Test Dataset\n\n// Function to combine Weather Data\ndef combineWeatherTest(Dataset1: DataFrame, Dataset2: DataFrame, Dataset3: DataFrame): DataFrame = {\n    \n    Dataset1.registerTempTable(\"weatherDataset1\");\n    Dataset2.registerTempTable(\"weatherDataset2\");\n    Dataset3.registerTempTable(\"weatherDataset3\");\n    \n    val weather_data_temp = spark.sql(\"select * from weatherDataset1 union select * from weatherDataset2 union select * from weatherDataset3\");\n\n    weather_data_temp.registerTempTable(\"combinedWeatherData\");\n\n    spark.sql(\"select * from combinedWeatherData where _c1=2008\");\n    \n}","user":"anonymous","dateUpdated":"2017-05-10T00:29:27-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nwarning: there were four deprecation warnings; re-run with -deprecation for details\n\ncombineWeatherTest: (Dataset1: org.apache.spark.sql.DataFrame, Dataset2: org.apache.spark.sql.DataFrame, Dataset3: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"}]},"apps":[],"jobName":"paragraph_1493592699469_-618404540","id":"20170430-185139_582282713","dateCreated":"2017-04-30T18:51:39-0400","dateStarted":"2017-05-10T00:30:01-0400","dateFinished":"2017-05-10T00:30:01-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:671"},{"text":"//Function to process FlightData\n//Only Considering JFK,LAX and SFO airports as the origin\n\ndef processFlightData(Dataset: DataFrame): DataFrame = {\n    \n    Dataset.registerTempTable(\"flightData\");\n    \n    spark.sql(\"select Year,Month, DayofMonth, DayofWeek, CAST((CRSDepTime/100) AS INT) as HourofDay, UniqueCarrier, Origin, Dest, Distance, CASE  WHEN DepDelay>15 THEN 1 WHEN DepDelay<=15 THEN 0 END AS LATE FROM flightData where Origin='JFK' OR Origin='LAX' OR Origin='SFO'\");\n}","user":"anonymous","dateUpdated":"2017-05-10T00:29:27-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nprocessFlightData: (Dataset: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"}]},"apps":[],"jobName":"paragraph_1493576724435_1786921198","id":"20170430-142524_764103919","dateCreated":"2017-04-30T14:25:24-0400","dateStarted":"2017-05-10T00:30:01-0400","dateFinished":"2017-05-10T00:30:02-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:672"},{"text":"// Function to process WeatherData\n\ndef processWeatherData(Dataset: DataFrame): DataFrame = {\n    \n    Dataset.registerTempTable(\"weatherDataset1\");\n    \n    spark.sql(\"select _c0, CAST(_c1 AS INT) as _c1, CAST(_c2 AS INT) as _c2, CAST(_c3 AS INT) as _c3, CAST(_c4 AS INT) as _c4, case when _c5 between 0 and 10 then 1 when _c5 between 10 and 20 then 2 when _c5 between 20 and 30 then 3 when  _c5 between 30 and 40 then 4 when _c5 between 40 and 50 then 5 when _c5 between 50 and 60 then 6 else 7 end as _c5, _c7, _c8 from weatherDataset1 where _c8 not in ('-DZ:01 FG:2 |FG:30 DZ:51 |FG:44 DZ:51','TWF.') and _c7 not in('0.00V', '2.00V', 'TWF.')\")\n    \n}\n","user":"anonymous","dateUpdated":"2017-05-10T00:29:27-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nprocessWeatherData: (Dataset: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"}]},"apps":[],"jobName":"paragraph_1493577477947_1080815545","id":"20170430-143757_1506485608","dateCreated":"2017-04-30T14:37:57-0400","dateStarted":"2017-05-10T00:30:01-0400","dateFinished":"2017-05-10T00:30:02-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:673"},{"text":"//Combining Flight and weather data\n\ndef combineFlightWeatherData(flightDataset: DataFrame, weatherDataset: DataFrame) : DataFrame = {\n    \n    flightDataset.registerTempTable(\"flightDataset\");\n    weatherDataset.registerTempTable(\"weatherDataset\");\n    \n    spark.sql(\"select * from flightDataset inner join weatherDataset on Year=_c1 and Month=_c2 and DayofMonth=_c3 and Origin=_c0 and HourOfDay=_c4 \");\n    \n} ","user":"anonymous","dateUpdated":"2017-05-10T00:29:27-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nwarning: there were two deprecation warnings; re-run with -deprecation for details\n\ncombineFlightWeatherData: (flightDataset: org.apache.spark.sql.DataFrame, weatherDataset: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"}]},"apps":[],"jobName":"paragraph_1493583360141_749125814","id":"20170430-161600_1612184978","dateCreated":"2017-04-30T16:16:00-0400","dateStarted":"2017-05-10T00:30:02-0400","dateFinished":"2017-05-10T00:30:02-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:674"},{"text":"/**After selecting the desired parameter */\n//Function to index categorical variables\n\ndef categoricalIndexing(Dataset: DataFrame): DataFrame = {\n    \n    val indexer = new StringIndexer()\n    .setInputCol(\"Origin\")\n    .setOutputCol(\"OriginIndex\")\n    .setHandleInvalid(\"skip\");\n    \n    val inputData1 = indexer.fit(Dataset).transform(Dataset);\n    \n    val indexer2 = new StringIndexer()\n    .setInputCol(\"Dest\")\n    .setOutputCol(\"DestIndex\")\n    .setHandleInvalid(\"skip\");\n    \n    val inputData2 = indexer2.fit(inputData1).transform(inputData1);\n    indexer2.fit(inputData1).transform(inputData1);\n    \n    val indexer3 = new StringIndexer()\n    .setInputCol(\"HourlyVisibility\")\n    .setOutputCol(\"HourlyVisibilityIndex\")\n    .setHandleInvalid(\"skip\");\n    \n    val inputData3 = indexer3.fit(inputData2).transform(inputData2);\n\n    val indexer4 = new StringIndexer()\n    .setInputCol(\"HourlyPrecip\")\n    .setOutputCol(\"HourlyPrecipIndex\")\n    .setHandleInvalid(\"skip\");\n    \n    indexer4.fit(inputData3).transform(inputData3);\n}","user":"anonymous","dateUpdated":"2017-05-10T00:29:27-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ncategoricalIndexing: (Dataset: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"}]},"apps":[],"jobName":"paragraph_1493583598649_-908435389","id":"20170430-161958_968292164","dateCreated":"2017-04-30T16:19:58-0400","dateStarted":"2017-05-10T00:30:02-0400","dateFinished":"2017-05-10T00:30:03-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:675"},{"text":"//Converting columns to Doubles\n\ndef convertToDouble(Dataset : DataFrame): DataFrame={\n    \n    \n    val toDouble = udf[Double, Int]( _.toDouble);\n    \n    Dataset.withColumn(\"Month\", toDouble(Dataset(\"Month\")))\n                    .withColumn(\"DayOfMonth\", toDouble(Dataset(\"DayOfMonth\")))\n                    .withColumn(\"DayOfWeek\", toDouble(Dataset(\"DayOfWeek\")))\n                    .withColumn(\"HourOfDay\", toDouble(Dataset(\"HourOfDay\")))\n                    .withColumn(\"Distance\", toDouble(Dataset(\"Distance\")))\n                    .withColumn(\"OriginIndex\", toDouble(Dataset(\"OriginIndex\")))\n                    .withColumn(\"DestIndex\", toDouble(Dataset(\"DestIndex\")))\n                    .withColumn(\"HourlyWindSpeed\", toDouble(Dataset(\"HourlyWindSpeed\")))\n                    .withColumn(\"HourlyVisibilityIndex\", toDouble(Dataset(\"HourlyVisibilityIndex\")))\n                    .withColumn(\"HourlyPrecipIndex\", toDouble(Dataset(\"HourlyPrecipIndex\")))\n                    .withColumn(\"LATE\", toDouble(Dataset(\"LATE\")))\n                    .select(\"Month\", \"DayOfMonth\", \"DayOfWeek\", \"HourOfDay\", \"Distance\", \"OriginIndex\", \"DestIndex\", \"HourlyWindSpeed\",\"HourlyVisibilityIndex\", \"HourlyPrecipIndex\",\"LATE\");\n}","user":"anonymous","dateUpdated":"2017-05-10T00:29:27-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nconvertToDouble: (Dataset: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"}]},"apps":[],"jobName":"paragraph_1493583847123_766512846","id":"20170430-162407_1977197457","dateCreated":"2017-04-30T16:24:07-0400","dateStarted":"2017-05-10T00:30:03-0400","dateFinished":"2017-05-10T00:30:05-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:676"},{"text":"// Loading the Flight Data\n\nval flight_2006 = loadData(\"/home/prateek/Downloads/2006.csv\", \"true\");\n\nval flight_2007 = loadData(\"/home/prateek/Downloads/2007.csv\", \"true\");\n\nval flight_2008 = loadData(\"/home/prateek/Downloads/2008.csv\", \"true\");","user":"anonymous","dateUpdated":"2017-05-10T00:29:28-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nflight_2006: org.apache.spark.sql.DataFrame = [Year: string, Month: string ... 27 more fields]\n\nflight_2007: org.apache.spark.sql.DataFrame = [Year: string, Month: string ... 27 more fields]\n\nflight_2008: org.apache.spark.sql.DataFrame = [Year: string, Month: string ... 27 more fields]\n"}]},"apps":[],"jobName":"paragraph_1493583975449_-962732601","id":"20170430-162615_61142741","dateCreated":"2017-04-30T16:26:15-0400","dateStarted":"2017-05-10T00:30:03-0400","dateFinished":"2017-05-10T00:30:10-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:677"},{"text":"// Load Weather Data\n\nval weather_jfk = loadData(\"/home/prateek/Downloads/JFK_weather2\", \"false\");\n\nval weather_lax = loadData(\"/home/prateek/Downloads/LAX_weather2\", \"false\");\n\nval weather_sf = loadData(\"/home/prateek/Downloads/SF_weather2\", \"false\");","user":"anonymous","dateUpdated":"2017-05-10T00:29:28-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nweather_jfk: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 11 more fields]\n\nweather_lax: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 11 more fields]\n\nweather_sf: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 11 more fields]\n"}]},"apps":[],"jobName":"paragraph_1493584860282_-454627612","id":"20170430-164100_1909808587","dateCreated":"2017-04-30T16:41:00-0400","dateStarted":"2017-05-10T00:30:05-0400","dateFinished":"2017-05-10T00:30:11-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:678"},{"text":"//Combining Flight Data\n\nval combinedFlightData = combineFlight(flight_2006, flight_2007);\n\nval flightData = processFlightData(combinedFlightData);\n","user":"anonymous","dateUpdated":"2017-05-10T00:29:28-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ncombinedFlightData: org.apache.spark.sql.DataFrame = [Year: string, Month: string ... 27 more fields]\n\nflightData: org.apache.spark.sql.DataFrame = [Year: string, Month: string ... 8 more fields]\n"}]},"apps":[],"jobName":"paragraph_1493584972398_-154337935","id":"20170430-164252_2144340427","dateCreated":"2017-04-30T16:42:52-0400","dateStarted":"2017-05-10T00:30:10-0400","dateFinished":"2017-05-10T00:30:13-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:679"},{"text":"//Combine Weather Data\n\nval combinedWeatherData = combineWeather(weather_jfk, weather_lax, weather_sf);\n\nval weatherData = processWeatherData(combinedWeatherData);","user":"anonymous","dateUpdated":"2017-05-10T00:29:28-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ncombinedWeatherData: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 11 more fields]\n\nweatherData: org.apache.spark.sql.DataFrame = [_c0: string, _c1: int ... 6 more fields]\n"}]},"apps":[],"jobName":"paragraph_1493585168859_-1310448728","id":"20170430-164608_1917067866","dateCreated":"2017-04-30T16:46:08-0400","dateStarted":"2017-05-10T00:30:11-0400","dateFinished":"2017-05-10T00:30:14-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:680"},{"text":"//Combine flightData with weatherData \n\nval flightWeatherData = combineFlightWeatherData(flightData, weatherData);\n\nflightWeatherData.registerTempTable(\"flightWeatherData\");","user":"anonymous","dateUpdated":"2017-05-10T00:29:28-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nflightWeatherData: org.apache.spark.sql.DataFrame = [Year: string, Month: string ... 16 more fields]\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"}]},"apps":[],"jobName":"paragraph_1493585591363_541749969","id":"20170430-165311_2082430318","dateCreated":"2017-04-30T16:53:11-0400","dateStarted":"2017-05-10T00:30:14-0400","dateFinished":"2017-05-10T00:30:15-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:681"},{"text":"//Selecting the desired Parameters\n\nval inputData = spark.sql(\"select Month, DayOfMonth, DayOfWeek, HourOfDay, Distance, Origin, Dest, UniqueCarrier, LATE, _c7 as HourlyVisibility, _c5 as HourlyWindSpeed, _c8 as HourlyPrecip from flightWeatherData where (_c7 is NOT NULL) and (_c5 is NOT NULL) and (_c8 is NOT NULL)\"); \n","user":"anonymous","dateUpdated":"2017-05-10T00:29:29-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ninputData: org.apache.spark.sql.DataFrame = [Month: string, DayOfMonth: string ... 10 more fields]\n"}]},"apps":[],"jobName":"paragraph_1493586030454_1567902645","id":"20170430-170030_1370769347","dateCreated":"2017-04-30T17:00:30-0400","dateStarted":"2017-05-10T00:30:14-0400","dateFinished":"2017-05-10T00:30:15-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:682"},{"text":"// Dealing with categorical variables\n\nval finalInputData = categoricalIndexing(inputData);\n\nfinalInputData.registerTempTable(\"finalInputData\");","user":"anonymous","dateUpdated":"2017-05-10T00:29:29-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nfinalInputData: org.apache.spark.sql.DataFrame = [Month: string, DayOfMonth: string ... 14 more fields]\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"}]},"apps":[],"jobName":"paragraph_1493586069764_-685525690","id":"20170430-170109_678683108","dateCreated":"2017-04-30T17:01:09-0400","dateStarted":"2017-05-10T00:30:15-0400","dateFinished":"2017-05-10T00:37:41-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:683"},{"text":"finalInputData.registerTempTable(\"finalInputData\");\n\nvar finalProcessedData = spark.sql(\"select CAST(Month as INT), CAST(DayOfMonth AS INT), CAST(DayOfWeek AS INT), CAST(HourOfDay AS INT), CAST(Distance AS INT), CAST(OriginIndex AS INT), CAST(DestIndex AS INT), CAST(HourlyWindSpeed AS INT), CAST(HourlyVisibilityIndex AS INT), CAST(HourlyPrecipIndex AS INT), CAST(LATE AS INT) from finalInputData where (Month is NOT NULL) AND (DayOfMonth is NOT NULL) AND (DayOfWeek is NOT NULL) AND (HourOfDay is NOT NULL) AND (Distance is NOT NULL) AND (OriginIndex is NOT NULL) AND (DestIndex is NOT NULL) AND (HourlyWindSpeed is NOT NULL) AND (HourlyVisibilityIndex is NOT NULL) AND (HourlyPrecipIndex is NOT NULL) AND (LATE is NOT NULL)\"); \n","user":"anonymous","dateUpdated":"2017-05-10T00:29:29-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nfinalProcessedData: org.apache.spark.sql.DataFrame = [Month: int, DayOfMonth: int ... 9 more fields]\n"}]},"apps":[],"jobName":"paragraph_1493586282125_1833731842","id":"20170430-170442_1894228788","dateCreated":"2017-04-30T17:04:42-0400","dateStarted":"2017-05-10T00:30:16-0400","dateFinished":"2017-05-10T00:37:42-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:684"},{"text":"// Converting training data to double value\n\nval trainData = convertToDouble(finalProcessedData);","user":"anonymous","dateUpdated":"2017-05-10T00:29:29-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ntrainData: org.apache.spark.sql.DataFrame = [Month: double, DayOfMonth: double ... 9 more fields]\n"}]},"apps":[],"jobName":"paragraph_1493586311236_-442427326","id":"20170430-170511_948291222","dateCreated":"2017-04-30T17:05:11-0400","dateStarted":"2017-05-10T00:37:42-0400","dateFinished":"2017-05-10T00:37:43-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:685"},{"text":"//Training Features from trainData\n\nval trainCategoricalFeatures = List(\"Month\", \"DayOfMonth\", \"DayOfWeek\", \"HourOfDay\", \"Distance\", \"OriginIndex\", \"DestIndex\", \"HourlyWindSpeed\", \"HourlyVisibilityIndex\", \"HourlyPrecipIndex\").map(trainData.columns.indexOf(_));\n\n\nval trainCategoricalTarget = List(\"LATE\").map(trainData.columns.indexOf(_));","user":"anonymous","dateUpdated":"2017-05-10T00:29:29-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ntrainCategoricalFeatures: List[Int] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n\ntrainCategoricalTarget: List[Int] = List(10)\n"}]},"apps":[],"jobName":"paragraph_1493586394620_1033408158","id":"20170430-170634_676609095","dateCreated":"2017-04-30T17:06:34-0400","dateStarted":"2017-05-10T00:37:42-0400","dateFinished":"2017-05-10T00:37:45-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:686"},{"text":"//Creation of Labeled Points\n\nval trainLabeledPoints = trainData.rdd.map(r => LabeledPoint(r.getDouble(trainCategoricalTarget(0).toInt), Vectors.dense(trainCategoricalFeatures.map(r.getDouble(_)).toArray)));","user":"anonymous","dateUpdated":"2017-05-10T00:29:29-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ntrainLabeledPoints: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[210] at map at <console>:46\n"}]},"apps":[],"jobName":"paragraph_1493586446939_1685228634","id":"20170430-170726_324265466","dateCreated":"2017-04-30T17:07:26-0400","dateStarted":"2017-05-10T00:37:43-0400","dateFinished":"2017-05-10T00:37:49-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:687"},{"text":"//Boosting Strategy for GBT\n\nval boostingStrategy = BoostingStrategy.defaultParams(\"Classification\");\n\nboostingStrategy.numIterations = 20;\n\nboostingStrategy.treeStrategy.numClasses = 2;\n\nboostingStrategy.treeStrategy.maxDepth = 5;\n\nboostingStrategy.treeStrategy.categoricalFeaturesInfo = Map[Int, Int]();","user":"anonymous","dateUpdated":"2017-05-10T00:29:29-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nboostingStrategy: org.apache.spark.mllib.tree.configuration.BoostingStrategy = BoostingStrategy(org.apache.spark.mllib.tree.configuration.Strategy@40895e96,org.apache.spark.mllib.tree.loss.LogLoss$@67f2c434,100,0.1,0.001)\n\nboostingStrategy.numIterations: Int = 20\n\nboostingStrategy.treeStrategy.numClasses: Int = 2\n\nboostingStrategy.treeStrategy.maxDepth: Int = 5\n\nboostingStrategy.treeStrategy.categoricalFeaturesInfo: Map[Int,Int] = Map()\n"}]},"apps":[],"jobName":"paragraph_1493577507303_215712838","id":"20170430-143827_524531747","dateCreated":"2017-04-30T14:38:27-0400","dateStarted":"2017-05-10T00:37:45-0400","dateFinished":"2017-05-10T00:37:50-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:688"},{"text":"//Training the gbt model\n\nval gbtModel = GradientBoostedTrees.train(trainLabeledPoints, boostingStrategy);\n","user":"anonymous","dateUpdated":"2017-05-10T00:29:30-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n\ngbtModel: org.apache.spark.mllib.tree.model.GradientBoostedTreesModel =\nTreeEnsembleModel classifier with 20 trees\n"}]},"apps":[],"jobName":"paragraph_1493577546862_-1982326979","id":"20170430-143906_79161652","dateCreated":"2017-04-30T14:39:06-0400","dateStarted":"2017-05-10T00:37:49-0400","dateFinished":"2017-05-10T00:43:16-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:689"},{"text":"// Hyper parameters for RandomForest\n\nimport org.apache.spark.mllib.tree.RandomForest\n\nval numClasses = 2\nval categoricalFeaturesInfo = Map[Int, Int]()\nval numTrees = 3\nval featureSubsetStrategy = \"auto\" // Let the algorithm choose.\nval impurity = \"gini\"\nval maxDepth = 4\nval maxBins = 32\n\nval RFmodel = RandomForest.trainClassifier(trainLabeledPoints, numClasses, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins);","user":"anonymous","dateUpdated":"2017-05-10T00:29:30-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.mllib.tree.RandomForest\n\nnumClasses: Int = 2\n\ncategoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()\n\nnumTrees: Int = 3\n\nfeatureSubsetStrategy: String = auto\n\nimpurity: String = gini\n\nmaxDepth: Int = 4\n\nmaxBins: Int = 32\n\n\nRFmodel: org.apache.spark.mllib.tree.model.RandomForestModel =\nTreeEnsembleModel classifier with 3 trees\n"}]},"apps":[],"jobName":"paragraph_1493689766840_149364787","id":"20170501-214926_1336890041","dateCreated":"2017-05-01T21:49:26-0400","dateStarted":"2017-05-10T00:37:50-0400","dateFinished":"2017-05-10T00:43:38-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:690"},{"text":"// Preparing testDataset\n\nval flightDataTest = processFlightData(flight_2008);\n\nval combinedWeatherDataTest = combineWeatherTest(weather_jfk, weather_lax, weather_sf);\n\nval weatherDataTest = processWeatherData(combinedWeatherDataTest);\n\nval flightWeatherDataTest = combineFlightWeatherData(flightDataTest, weatherDataTest);\n\nflightWeatherDataTest.registerTempTable(\"flightWeatherDataTest\");\n\nval inputDataTest = spark.sql(\"select Month, DayOfMonth, DayOfWeek, HourOfDay, Distance, Origin, Dest, UniqueCarrier, _c5 as HourlyWindSpeed,_c7 as HourlyVisibility, _c8 as HourlyPrecip, LATE from flightWeatherDataTest where (_c5 is NOT NULL) AND (_c7 is NOT NULL) AND (_c8 is NOT NULL)\"); \n\nval finalInputDataTest = categoricalIndexing(inputDataTest);\n\nfinalInputDataTest.registerTempTable(\"finalInputDataTest\");\n\nvar finalProcessedDataTest = spark.sql(\"select CAST(Month as INT), CAST(DayOfMonth AS INT), CAST(DayOfWeek AS INT), CAST(HourOfDay AS INT), CAST(Distance AS INT), CAST(OriginIndex AS INT), CAST(DestIndex AS INT), CAST(HourlyWindSpeed AS INT), CAST(HourlyVisibilityIndex AS INT), CAST(HourlyPrecipIndex AS INT), CAST(LATE AS INT) from finalInputDataTest where (Month is NOT NULL) AND (DayOfMonth is NOT NULL) AND (DayOfWeek is NOT NULL) AND (HourOfDay is NOT NULL) AND (Distance is NOT NULL) AND (OriginIndex is NOT NULL) AND (DestIndex is NOT NULL) AND (HourlyWindSpeed is NOT NULL) AND (HourlyVisibilityIndex is NOT NULL) AND (HourlyPrecipIndex is NOT NULL) AND (LATE is NOT NULL)\"); \n\n\nval testData = convertToDouble(finalProcessedDataTest);\n\nval testCategoricalFeatures = List(\"Month\", \"DayOfMonth\", \"DayOfWeek\", \"HourOfDay\", \"Distance\", \"OriginIndex\", \"DestIndex\", \"HourlyWindSpeed\", \"HourlyVisibilityIndex\", \"HourlyPrecipIndex\").map(testData.columns.indexOf(_));\n\nval testCategoricalTarget = List(\"LATE\").map(testData.columns.indexOf(_));\n\nval testLabeledPoints = testData.rdd.map(r => LabeledPoint(r.getDouble(testCategoricalTarget(0).toInt), Vectors.dense(testCategoricalFeatures.map(r.getDouble(_)).toArray)));","user":"anonymous","dateUpdated":"2017-05-10T00:29:30-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nflightDataTest: org.apache.spark.sql.DataFrame = [Year: string, Month: string ... 8 more fields]\n\ncombinedWeatherDataTest: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 11 more fields]\n\nweatherDataTest: org.apache.spark.sql.DataFrame = [_c0: string, _c1: int ... 6 more fields]\n\nflightWeatherDataTest: org.apache.spark.sql.DataFrame = [Year: string, Month: string ... 16 more fields]\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\ninputDataTest: org.apache.spark.sql.DataFrame = [Month: string, DayOfMonth: string ... 10 more fields]\n\nfinalInputDataTest: org.apache.spark.sql.DataFrame = [Month: string, DayOfMonth: string ... 14 more fields]\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nfinalProcessedDataTest: org.apache.spark.sql.DataFrame = [Month: int, DayOfMonth: int ... 9 more fields]\n\ntestData: org.apache.spark.sql.DataFrame = [Month: double, DayOfMonth: double ... 9 more fields]\n\ntestCategoricalFeatures: List[Int] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n\ntestCategoricalTarget: List[Int] = List(10)\n\ntestLabeledPoints: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[898] at map at <console>:45\n"}]},"apps":[],"jobName":"paragraph_1493577554589_-551045747","id":"20170430-143914_2061508799","dateCreated":"2017-04-30T14:39:14-0400","dateStarted":"2017-05-10T00:43:17-0400","dateFinished":"2017-05-10T00:46:03-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:691"},{"text":"//TestError in GBT\n\nval labelAndPreds = testLabeledPoints.map { point =>\n  val prediction = gbtModel.predict(point.features)\n  (point.label, prediction)\n}\nval testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testLabeledPoints.count()\n//println(\"Learned classification GBT model:\\n\" + gbtModel.toDebugString)\nprintln(\"Test Error = \" + testErr)","user":"anonymous","dateUpdated":"2017-05-10T00:29:30-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nlabelAndPreds: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[899] at map at <console>:63\n\ntestErr: Double = 0.24048924425910584\nTest Error = 0.24048924425910584\n"}]},"apps":[],"jobName":"paragraph_1493593101983_-221180566","id":"20170430-185821_1101662421","dateCreated":"2017-04-30T18:58:21-0400","dateStarted":"2017-05-10T00:43:38-0400","dateFinished":"2017-05-10T00:47:02-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:692"},{"text":"//TestError in Random Forest\n\nval labelAndPreds = testLabeledPoints.map { point =>\n  val prediction = RFmodel.predict(point.features)\n  (point.label, prediction)\n}\nval testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testLabeledPoints.count()\n//println(\"Learned classification GBT model:\\n\" + gbtModel.toDebugString)\nprintln(\"Test Error = \" + testErr)","user":"anonymous","dateUpdated":"2017-05-10T00:29:30-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nlabelAndPreds: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[901] at map at <console>:75\n\ntestErr: Double = 0.25787714905381126\nTest Error = 0.25787714905381126\n"}]},"apps":[],"jobName":"paragraph_1493593166834_-315535992","id":"20170430-185926_1142121736","dateCreated":"2017-04-30T18:59:26-0400","dateStarted":"2017-05-10T00:46:03-0400","dateFinished":"2017-05-10T00:47:58-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:693"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1493755334996_1989789444","id":"20170502-160214_1339851742","dateCreated":"2017-05-02T16:02:14-0400","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:695"}],"name":"DelayPredictionCode","id":"2CGK8RCD2","angularObjects":{"2CBD594KV:shared_process":[],"2CDE76KUN:shared_process":[],"2CBPUPUQU:shared_process":[],"2CB93PDZB:shared_process":[],"2CC3448N2:shared_process":[],"2CAK8UWH4:shared_process":[],"2CC1DBAH9:shared_process":[],"2CAPKY798:shared_process":[],"2CAMW71J7:shared_process":[],"2CARHHKKR:shared_process":[],"2CA4MQK7J:shared_process":[],"2C9YC3V5H:shared_process":[],"2CBZ54A5T:shared_process":[],"2CA2FT3WB:shared_process":[],"2CDMMCJFY:shared_process":[],"2CCGZH759:shared_process":[],"2CC2B36D8:shared_process":[],"2CDKF9R4M:shared_process":[],"2CAFYMQRS:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}